version: '3.3'
services:
#  inference:
#    image: tensorflow/serving
#    ports:
#      - '8501:8501'
#    volumes:
#      - './inference/models:/models'
#    command:
#      - '--model_config_file=/models/models.config'
#      - '--model_config_file_poll_wait_seconds=60'
  gateway:
    build:
      context: gateway
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
